# This file is a part of EuclidianNormalizingFlows.jl, licensed under the MIT License (MIT)


# Non-public:
#=
    middle_deriv_2ndorder_poly(x::NTuple{3,Real}, y::NTuple{3,Real})

    middle_deriv_2ndorder_poly(
        x1::Real, x2::Real, x3::Real,
        y1::Real, y2::Real, y3::Real
    )

Returns derivative dy/dx of the 2nd order polynomial through the
three points (x[i], y[i]) at x[2].

Requires x[1] < x[2] < x[3].
=#
function middle_deriv_2ndorder_poly end

function middle_deriv_2ndorder_poly(x::NTuple{3,Real}, y::NTuple{3,Real})
    # Math generated by VS Code CoPilot.
    # Checked via
    # ForwardDiff.derivative(Polynomials.fit([x...], [y...], 2), x[2])

    x1, x2, x3 = x
    y1, y2, y3 = y
    @assert x1 < x2 < x3

    denom = (x1 - x2) * (x1 - x3) * (x2 - x3)
    inv_denom = inv(denom)
    a = (y1 * (x2 - x3) + y2 * (x3 - x1) + y3 * (x1 - x2)) * inv_denom
    b = (y1 * (x3^2 - x2^2) + y2 * (x1^2 - x3^2) + y3 * (x2^2 - x1^2))  * inv_denom
    # c = (y_l * (x_m * x_r * (x_m - x_r)) + y_m * (x_r * x_l * (x_r - x_l)) + y_r * (x_l * x_m * (x_l - x_m))) * inv_denom

    return 2a * x2 + b
end

middle_deriv_2ndorder_poly(x1::Real, x2::Real, x3::Real, y1::Real, y2::Real, y3::Real) =
    middle_deriv_2ndorder_poly((x1, x2, x3), (y1, y2, y3))


"""
    MonotonicSplines.estimate_dYdX(pX::AbstractVector{<:Real}, pY::AbstractVector{<:Real})

Get estimates for derivatives at the given knot points `pX`, `pY`.

The resulting derivatives can be used directly, but are also useful as initial
values, when optimizing spline derivative parameters based on
application-specific criteria.

Uses a linear estimate for the first and last knot and a 2nd order
polynomial estimate over three knots for the other knots.

Example:

```julia
dYdX = MonotonicSplines.estimate_dYdX(pX, pY)
f = RQSpline(pX, pY, dYdX)
```

When an RQSpline is intended to be used beyong the first and last knot, the
first and last derivative should be one, so in that case use

```julia
f = RQSpline(pX, pY, vcat(1, dYdX[begin+1:end-1], 1))
```
"""
function estimate_dYdX(pX::AbstractVector{<:Real}, pY::AbstractVector{<:Real})
    deriv_est_first = (pY[begin+1] -  pY[begin]) / (pX[begin+1] - pX[begin])
    deriv_est = middle_deriv_2ndorder_poly.(
        pX[begin:end-2], pX[begin+1:end-1], pX[begin+2:end],
        pY[begin:end-2], pY[begin+1:end-1], pY[begin+2:end]
    )
    deriv_est_last = (pY[end] -  pY[end-1]) / (pX[end] - pX[end-1])
    return vcat(deriv_est_first, deriv_est, deriv_est_last)
end


"""
    MonotonicSplines.rqs_params_from_nn(θ_raw::AbstractArray, n_dims_trafo::Integer, B::Real = 5.)

Process the raw output parameters of a neural network to generate parameters for a set of rational quadratic spline functions.

# Arguments
- `θ_raw`: A matrix where each column represents the raw parameters for a sample.
- `n_dims_trafo`: The number of spline functions for which parameters are to be produced.
- `B`: Sets the rage of the splines.

# Returns
- A tuple `pX, pY, dYdX` containing the positions of and derivatives at the spline knots.
  The parameters are stored in a `K+1 x n_spline_functions_per_sample x n_samples` array.
"""
function rqs_params_from_nn(θ_raw::AbstractArray, n_dims_trafo::Integer, B::Real = 5.)
    N = size(θ_raw, 2)
    K = Int((size(θ_raw,1)/n_dims_trafo+1)/3)
    θ = reshape(θ_raw, :, n_dims_trafo, N)

    compute_unit = get_compute_unit(θ_raw)

    pX =  cat(adapt(compute_unit, repeat([-B], 1, n_dims_trafo, N)), _cumsum_tri(_softmax_tri(θ[1:K,:,:])); dims = 1)
    pY =  cat(adapt(compute_unit, repeat([-B], 1, n_dims_trafo, N)), _cumsum_tri(_softmax_tri(θ[K+1:2K,:,:])); dims = 1)
    dYdX =  cat(adapt(compute_unit, repeat([1], 1, n_dims_trafo, N)), _softplus_tri(θ[2K+1:end,:,:]), adapt(compute_unit, repeat([1], 1, n_dims_trafo, N)); dims = 1)

    return pX, pY, dYdX
end


# Non-public:
#=
    _sort_dimensions(y₁::AbstractArray, y₂::AbstractArray, mask::AbstractVector)

Create a new array by selectively replacing rows from `y₂` with corresponding rows from `y₁` based on a boolean mask, `mask`.

# Arguments
- `y₁`: An array from which rows are taken. It should have the same number of columns as `y₂`.
- `y₂`: An array that serves as the base for the output. Rows specified by `mask` are replaced with corresponding rows from `y₁`.
- `mask`: A boolean vector of the same length as the number of rows in `y₁` and `y₂`. If the i-th element of `mask` is true, the i-th row of `y₂` is replaced with the i-th row of `y₁` in the output.

# Returns
- `res`: An array of the same shape as `y₂`, but with rows specified by `mask` replaced with corresponding rows from `y₁`.
=#
function _sort_dimensions(y₁::AbstractArray, y₂::AbstractArray, mask::AbstractVector)
    
    if mask[1]
        res = reshape(y₁[1,:],1,size(y₁,2))
        c=2
    else
        res = reshape(y₂[1,:],1,size(y₁,2))
        c=1
    end

    for (i,b) in enumerate(mask[2:end])
        if b
            res = vcat(res, reshape(y₁[c,:],1,size(y₁,2)))
            c+=1
        else
            res = vcat(res, reshape(y₂[i+1,:],1,size(y₂,2)))
        end
    end

    return res
end

function _softmax(x::AbstractVector)

    exp_x = exp.(x)
    sum_exp_x = sum(exp_x)

    return exp_x ./ sum_exp_x 
end

function _softmax(x::AbstractMatrix)

    val = cat([_softmax(i) for i in eachrow(x)]..., dims=2)'

    return val 
end

function _softmax_tri(x::AbstractArray)
    exp_x = exp.(x)
    inv_sum_exp_x = inv.(sum(exp_x, dims = 1))

    return inv_sum_exp_x .* exp_x
end

function _cumsum(x::AbstractVector; B = 5)
    return 2 .* B .* cumsum(x) .- B 
end

function _cumsum(x::AbstractMatrix)

    return cat([_cumsum(i) for i in eachrow(x)]..., dims=2)'
end

function _cumsum_tri(x::AbstractArray, B::Real = 5.)
    
    return 2 .* B .* cumsum(x, dims = 1) .- B 
end

function _softplus(x::AbstractVector)

    return log.(exp.(x) .+ 1) 
end

function _softplus(x::AbstractMatrix)

    val = cat([_softplus(i) for i in eachrow(x)]..., dims=2)'

    return val
end

function _softplus_tri(x::AbstractArray)
    return log.(exp.(x) .+ 1) 
end


midpoint(lo::T, hi::T) where T<:Integer = lo + ((hi - lo) >>> 0x01)
binary_log(x::T) where {T<:Integer} = 8 * sizeof(T) - leading_zeros(x - 1)

function searchsortedfirst_impl(
        v::AbstractVector, 
        x::Real
    )
    
    u = one(Integer)
    lo = one(Integer) - u
    hi = length(v) + u
    
    n = binary_log(length(v))+1
    m = one(Integer)
    
    @inbounds for i in 1:n
        m_1 = midpoint(lo, hi)
        m = Base.ifelse(lo < hi - u, m_1, m)
        lo = Base.ifelse(v[m] < x, m, lo)
        hi = Base.ifelse(v[m] < x, hi, m)
    end
    return hi
end


_ka_threads(::KernelAbstractions.CPU) = (Base.Threads.nthreads(),)
_ka_threads(::KernelAbstractions.Backend) = ()
